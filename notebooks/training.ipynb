{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fitting\n",
    "\n",
    "In this file, we will experiment with different prediction models in order to see if we can create a model that will accurately predict if a Pokemon is a legendary or not.\n",
    "\n",
    "Models Used: Decision Tree, Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data imports\n",
    "pokemon_data = pd.read_csv(\"../data/pokemon_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal for our model, is for the model to be able to accurately predict if a given Pokemon is a legendary or not, using other properties (variates) of the Pokemon. \n",
    "\n",
    "For this, we will use (and compare) 3 different Models: Decision Tree, Random Forest, and XGBoost.\n",
    "\n",
    "At the time of writing (Jan 30th 2024), there have been 9 generations of Pokemon with their own set of legendaries. So we will use those legendaries to also test to see if our model is accurate with future data!\n",
    "\n",
    "Let's start by splitting our data into test data and training data and fit a Decision Tree. For this model, we will remove all unique attributes (such as name, classification, japanese_name, abilities) and also encode types using 25 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014925373134328358\n"
     ]
    }
   ],
   "source": [
    "y = pokemon_data[\"is_legendary\"]\n",
    "X = pokemon_data.loc[:, pokemon_data.columns != \"is_legendary\"]\n",
    "X = X.loc[:, X.columns != \"abilities\"]\n",
    "X = X.loc[:, X.columns != \"name\"]\n",
    "X = X.loc[:, X.columns != \"classfication\"]\n",
    "X = X.loc[:, X.columns != \"japanese_name\"]\n",
    "s = (X.dtypes == \"object\")\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 1)\n",
    "\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train_X[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.fit_transform(val_X[object_cols]))\n",
    "\n",
    "OH_cols_train.index = train_X.index\n",
    "OH_cols_valid.index = val_X.index\n",
    "\n",
    "num_X_train = train_X.drop(object_cols, axis=1)\n",
    "num_X_valid = val_X.drop(object_cols, axis=1)\n",
    "\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "poke_model = DecisionTreeRegressor()\n",
    "poke_model.fit(OH_X_train.values, train_y)\n",
    "val_predictions = poke_model.predict(OH_X_valid.values)\n",
    "print(mean_absolute_error(val_y, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get a MAE of approximately 0.0149. Now let's try to use RandomForests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014776119402985073\n"
     ]
    }
   ],
   "source": [
    "forest_model_full = RandomForestRegressor(random_state=1)\n",
    "forest_model_full.fit(OH_X_train.values, train_y)\n",
    "forest_full_predict = forest_model_full.predict(OH_X_valid.values)\n",
    "print(mean_absolute_error(val_y, forest_full_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is marginally better using the full model (by approx 0.0002). \n",
    "\n",
    "Now that we've looked at the full models, let's see what we can do using only specific features.\n",
    "\n",
    "When I think of which factors could affect the legendary predictions, I think of the following:\n",
    "- Base Stat Totals (they should be high for legendaries)\n",
    "- Types (certain type-combos are more likely to have legendaries, see explore_data)\n",
    "- Height and Weight\n",
    "- Generation\n",
    "\n",
    "Let's try to fit Decision Tree and RandomForests Models using these features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05223880597014925\n"
     ]
    }
   ],
   "source": [
    "cols_to_use = [\"base_total\", \"type1\", \"type2\", \"generation\"]\n",
    "\n",
    "X_reduced = pokemon_data[cols_to_use]\n",
    "\n",
    "train_X_r, val_X_r, train_y_r, val_y_r = train_test_split(X_reduced, y, random_state = 1)\n",
    "\n",
    "OH_encoder_R = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train_R = pd.DataFrame(OH_encoder_R.fit_transform(train_X_r[object_cols]))\n",
    "OH_cols_valid_R= pd.DataFrame(OH_encoder_R.fit_transform(val_X_r[object_cols]))\n",
    "\n",
    "OH_cols_train_R.index = train_X_r.index\n",
    "OH_cols_valid_R.index = val_X_r.index\n",
    "\n",
    "num_X_train_r = train_X_r.drop(object_cols, axis=1)\n",
    "num_X_valid_r = val_X_r.drop(object_cols, axis=1)\n",
    "\n",
    "OH_X_train_r = pd.concat([num_X_train_r, OH_cols_train_R], axis=1)\n",
    "OH_X_valid_r = pd.concat([num_X_valid_r, OH_cols_valid_R], axis=1)\n",
    "\n",
    "tree_r = DecisionTreeRegressor()\n",
    "tree_r.fit(OH_X_train_r.values, train_y_r)\n",
    "val_predictions_r = tree_r.predict(OH_X_valid_r.values)\n",
    "print(mean_absolute_error(val_y_r, val_predictions_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not an improvement compared to the full model. It actually got worse. Let's see if we can play around with the lead_nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max leaf nodes: 5  \t\t Mean Absolute Error:  0.058560\n",
      "Max leaf nodes: 6  \t\t Mean Absolute Error:  0.060071\n",
      "Max leaf nodes: 7  \t\t Mean Absolute Error:  0.060624\n",
      "Max leaf nodes: 8  \t\t Mean Absolute Error:  0.060624\n",
      "Max leaf nodes: 9  \t\t Mean Absolute Error:  0.059522\n",
      "Max leaf nodes: 10  \t\t Mean Absolute Error:  0.057294\n",
      "Max leaf nodes: 15  \t\t Mean Absolute Error:  0.054263\n",
      "Max leaf nodes: 20  \t\t Mean Absolute Error:  0.061325\n",
      "Max leaf nodes: 100  \t\t Mean Absolute Error:  0.054726\n"
     ]
    }
   ],
   "source": [
    "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=1)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds_val = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, preds_val)\n",
    "    return(mae)\n",
    "\n",
    "for max_leaf_nodes in [5, 6, 7, 8, 9, 10, 15, 20, 100]:\n",
    "    my_mae = get_mae(max_leaf_nodes, OH_X_train_r.values, OH_X_valid_r.values, train_y_r, val_y_r)\n",
    "    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %f\" %(max_leaf_nodes, my_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like we cannot improve on the model using Decision Trees.\n",
    "\n",
    "So let's try using RandomForests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04820066334991708\n"
     ]
    }
   ],
   "source": [
    "forest_model_reduced = RandomForestRegressor(random_state=1)\n",
    "forest_model_reduced.fit(OH_X_train_r.values, train_y_r)\n",
    "forest_reduced_predict = forest_model_reduced.predict(OH_X_valid_r.values)\n",
    "print(mean_absolute_error(val_y_r, forest_reduced_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that it is still not as good as the model with the full model. So using these tools, the best model is still the full model using RandomForests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future, I want to try doing:\n",
    "- Feature Engineering\n",
    "- Cross-Validation\n",
    "- XGBoost\n",
    "- Compare the model to data from future generations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
